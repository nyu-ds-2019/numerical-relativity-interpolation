{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "\n",
    "from pytorch_lightning import Trainer, seed_everything\n",
    "from pytorch_lightning.callbacks.early_stopping import EarlyStopping\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "\n",
    "from models.unet3d.model import PlaceholderModel\n",
    "from models.unet3d.loader import SingleChannelDataset\n",
    "\n",
    "from pathlib import Path\n",
    "from argparse import Namespace\n",
    "\n",
    "import tqdm\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dir(dir_path):\n",
    "    if not os.path.exists(dir_path):\n",
    "        os.makedirs(dir_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "userid1 = 'ns4486'\n",
    "userid2 = 'prs392'\n",
    "model_num = 'NUM-151'\n",
    "\n",
    "run_through_model = False\n",
    "generate_input_images = True\n",
    "generate_output_images = True\n",
    "generate_videos = True\n",
    "\n",
    "inputs_outputs_save_path = f'/scratch/{userid1}/capstone/results/{model_num}.npy'\n",
    "input_images_path = f'/scratch/{userid2}/capstone/simulations/{model_num}/original_35th_frames_scaled'\n",
    "output_images_path = f'/scratch/{userid2}/capstone/simulations/{model_num}/predict_35th_frames_model/'\n",
    "video_dir = f'/scratch/{userid2}/capstone/simulations/videos/{model_num}'\n",
    "all_expected_predicted_video_path = f'{video_dir}/all_expected_predicted.avi'\n",
    "choppy_expected_predicted_video_path = f'{video_dir}/choppy_expected_predicted.avi'\n",
    "\n",
    "create_dir(video_dir)\n",
    "create_dir(input_images_path)\n",
    "create_dir(output_images_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/scratch/prs392/capstone/simulations/videos/NUM-151/choppy_expected_predicted.avi'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "choppy_expected_predicted_video_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "if run_through_model:\n",
    "\n",
    "    checkpoint_dir = f\"/scratch/ns4486/capstone/checkpoints/unet3d/arch_1/unet3d_batch1/{model_num}/checkpoints/last.ckpt\"\n",
    "    data_path = '/scratch/ns4486/numerical-relativity-interpolation/Proca_fiducial_scaled_cropped.hdf5'\n",
    "\n",
    "    model = PlaceholderModel.load_from_checkpoint(checkpoint_dir, hparams = {}, data_path=data_path)\n",
    "\n",
    "    model = model.eval()\n",
    "\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    model = model.to(device)\n",
    "\n",
    "    dataset = SingleChannelDataset(data_path)\n",
    "\n",
    "    outputs = None\n",
    "    inputs = None\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm.tqdm(range(len(dataset))):\n",
    "            x1 = dataset[i][0].unsqueeze(0)\n",
    "            x2 = dataset[i][1].unsqueeze(0)\n",
    "            \n",
    "            x = torch.cat([x1, x2], dim=1)\n",
    "            x = x.to(device)\n",
    "            \n",
    "            y_hat = model(x)\n",
    "\n",
    "            if inputs is None:\n",
    "                inputs = torch.cat((dataset[i][0], dataset[i][1]))\n",
    "                inputs = inputs.unsqueeze(0)\n",
    "            else:\n",
    "                t = torch.cat((dataset[i][0], dataset[i][1])).unsqueeze(0)\n",
    "                inputs = torch.cat((inputs, t))\n",
    "\n",
    "            if outputs is None:\n",
    "                outputs = y_hat.cpu().squeeze(0).detach().numpy()\n",
    "            else:\n",
    "                outputs = np.concatenate((outputs, y_hat.cpu().squeeze(0).detach().numpy()), axis=0)\n",
    "\n",
    "    outputs = outputs.reshape((407, 1, 54, 54, 54))\n",
    "    inputs = np.array(inputs)\n",
    "\n",
    "    with open(inputs_outputs_save_path, 'wb') as f:\n",
    "        np.save(f, inputs)\n",
    "        np.save(f, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# outputs = outputs.reshape((407, 1, 54, 54, 54))\n",
    "# inputs = np.array(inputs)\n",
    "\n",
    "# with open(inputs_outputs_save_path, 'wb') as f:\n",
    "#     np.save(f, inputs)\n",
    "#     np.save(f, outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs_2 = inputs[:, :, 9:63, 9:63, 9:63]\n",
    "# inputs_2[:, 0, 15, :, :]\n",
    "# inputs_2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_input_images or generate_output_images:\n",
    "\n",
    "    with open(inputs_outputs_save_path, 'rb') as f:\n",
    "        inputs = np.load(f)\n",
    "        outputs = np.load(f)\n",
    "    \n",
    "    cats = pd.qcut(inputs[:, 0, 35, :, :].flatten(), q=30)\n",
    "    \n",
    "    inputs = inputs[:, :, 9:63, 9:63, 9:63]\n",
    "    \n",
    "    new_input = inputs[:, 0, 26, 11:43, 11:43]\n",
    "    new_output = outputs[:, 0, 26, 11:43, 11:43]\n",
    "\n",
    "#     cats = pd.qcut(new_input.flatten(), q=30)\n",
    "\n",
    "    color_interval_values = []\n",
    "    color = 1\n",
    "\n",
    "    for interval in list(cats.categories):\n",
    "        color_interval_values.append([interval.left, interval.right, color])\n",
    "        color += 1\n",
    "\n",
    "    color_intervals_df = pd.DataFrame(color_interval_values, columns=['left', 'right', 'color'])\n",
    "    color_intervals_df\n",
    "\n",
    "    def estimate_color(x):\n",
    "        for index, row in color_intervals_df.iterrows():\n",
    "            if x > row.left and x <= row.right:\n",
    "                return row.color\n",
    "        if x <= color_intervals_df.iloc[0]['left']:\n",
    "            return 1\n",
    "        else:\n",
    "            return 30\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407/407 [07:52<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "if generate_input_images:\n",
    "\n",
    "    for index in tqdm.tqdm(range(new_input.shape[0])):\n",
    "        location_value = []\n",
    "        frame = 0\n",
    "\n",
    "        for i in range(new_input[index].shape[0]):\n",
    "            for j in range(new_input[index].shape[1]):\n",
    "                location_value.append([i, j, new_input[index, i, j]])\n",
    "\n",
    "        df = pd.DataFrame(data = location_value, columns=['x', 'y', 'value'])\n",
    "\n",
    "        df['color'] = df['value'].apply(estimate_color)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        plt.scatter(df['x'], df['y'], c=df['color'], vmin=1, vmax=30, s=300)\n",
    "        \n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        plt.plot()\n",
    "#         if index == 10:\n",
    "#             break\n",
    "\n",
    "        plt.savefig(os.path.join(input_images_path, f'{index}.png'), dpi = 50)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407/407 [07:51<00:00,  1.16s/it]\n"
     ]
    }
   ],
   "source": [
    "if generate_output_images:\n",
    "\n",
    "    for index in tqdm.tqdm(range(new_output.shape[0])):\n",
    "        location_value = []\n",
    "        frame = 0\n",
    "\n",
    "        for i in range(new_output[index].shape[0]):\n",
    "            for j in range(new_output[index].shape[1]):\n",
    "                location_value.append([i, j, new_output[index, i, j]])\n",
    "\n",
    "        df = pd.DataFrame(data = location_value, columns=['x', 'y', 'value'])\n",
    "\n",
    "        df['color'] = df['value'].apply(estimate_color)\n",
    "\n",
    "#         fig = px.scatter(df, x='x', y='y', color='color', range_color = [1, 30], color_continuous_scale='Viridis', width=800, height=800)\n",
    "\n",
    "#         fig.update_traces(marker=dict(size=12), selector=dict(mode='markers'))\n",
    "\n",
    "#         fig.write_image(os.path.join(output_images_path, f'{index}.png'), width=800, height=800)\n",
    "        \n",
    "        plt.figure(figsize=(10, 8))\n",
    "        \n",
    "        plt.scatter(df['x'], df['y'], c=df['color'], vmin=1, vmax=30, s=300)\n",
    "        \n",
    "        plt.colorbar()\n",
    "        plt.axis('off')\n",
    "        plt.plot()\n",
    "\n",
    "        plt.savefig(os.path.join(output_images_path, f'{index}.png'), dpi = 50)\n",
    "        plt.close('all')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "if generate_videos:\n",
    "\n",
    "    original_images = [f'{i}.png' for i in range(407)]\n",
    "    original_images = original_images[1:]\n",
    "\n",
    "    predicted_images = [f'{i}.png' for i in range(407)]\n",
    "    predicted_images = predicted_images[:-1]\n",
    "\n",
    "\n",
    "    frame = cv2.imread(os.path.join(input_images_path, original_images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(all_expected_predicted_video_path, 0, 10, (width*2,height))\n",
    "\n",
    "    for i in range(len(original_images)):\n",
    "        # change on every other image\n",
    "        left_img = cv2.imread(os.path.join(input_images_path, original_images[i]))\n",
    "\n",
    "        right_img = cv2.imread(os.path.join(output_images_path, predicted_images[i]))\n",
    "\n",
    "        im_h = cv2.hconcat([left_img, right_img])\n",
    "\n",
    "        video.write(im_h)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()\n",
    "\n",
    "\n",
    "    original_images = [f'{i}.png' for i in range(407)]\n",
    "    predicted_images = [f'{i}.png' for i in range(407)]\n",
    "\n",
    "    frame = cv2.imread(os.path.join(input_images_path, original_images[0]))\n",
    "    height, width, layers = frame.shape\n",
    "\n",
    "    video = cv2.VideoWriter(choppy_expected_predicted_video_path, 0, 10, (width*3,height))\n",
    "\n",
    "    for i in range(len(original_images)):\n",
    "        # change on every other image\n",
    "        if i % 2 == 0:\n",
    "            left_img = cv2.imread(os.path.join(input_images_path, original_images[i]))\n",
    "\n",
    "            right_img = left_img\n",
    "        else:\n",
    "            right_img = cv2.imread(os.path.join(output_images_path, predicted_images[i - 1]))\n",
    "\n",
    "        mid_img = cv2.imread(os.path.join(input_images_path, original_images[i]))\n",
    "\n",
    "        im_h = cv2.hconcat([left_img, mid_img, right_img])\n",
    "\n",
    "        video.write(im_h)\n",
    "\n",
    "    cv2.destroyAllWindows()\n",
    "    video.release()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
